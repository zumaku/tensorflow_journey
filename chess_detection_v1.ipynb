{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6IEy2sSlsmL1i2jZ4NP72",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zumaku/tensorflow_journey/blob/main/chess_detection_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 1: Persiapan Lingkungan di Google Colab\n",
        "Pertama, kita akan memulai dengan mengimpor pustaka yang dibutuhkan dan menghubungkan Google Colab dengan Google Drive."
      ],
      "metadata": {
        "id": "2vaVUazhWabF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyh1m3IbRBfh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghubungkan Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izGGM5lcVuzI",
        "outputId": "b27e717a-50c7-477c-a930-661b99f3db5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menentukan direktori dataset\n",
        "dataset_dir = '/content/drive/MyDrive/Tensorflow Journey/Chessman-image-dataset'"
      ],
      "metadata": {
        "id": "fdsarfGcV1rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 2: Mempersiapkan Dataset\n",
        "Setelah itu, kita akan mempersiapkan dataset untuk digunakan dalam pelatihan model."
      ],
      "metadata": {
        "id": "nn2MH6lzWd_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alur Kerja**\n",
        "1. Normalisasi:\n",
        "  \n",
        "  Gambar yang dimuat akan dinormalisasi dengan skala 1.0/255.0 sehingga nilai piksel berada dalam rentang 0-1.\n",
        "\n",
        "2. Pembagian Dataset:\n",
        "\n",
        "  Dataset akan secara otomatis dibagi menjadi subset pelatihan dan validasi berdasarkan parameter validation_split.\n",
        "\n",
        "3. Batch Processing:\n",
        "\n",
        "  Gambar akan diproses dalam batch (32 gambar per batch) untuk efisiensi memori dan performa pelatihan.\n",
        "\n",
        "4. Pengubahan Ukuran Gambar:\n",
        "\n",
        "  Setiap gambar akan diubah ukurannya menjadi 150x150 piksel sebelum dimasukkan ke dalam model.\n",
        "\n",
        "5. One-Hot Encoding:\n",
        "\n",
        "  Label kelas gambar akan diubah menjadi one-hot encoded vectors untuk digunakan dalam klasifikasi multi-kelas."
      ],
      "metadata": {
        "id": "K-eY0fClaKnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatur parameter ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    validation_split=0.2  # 20% data digunakan untuk validasi\n",
        ")"
      ],
      "metadata": {
        "id": "6pZrEMWSWfbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ImageDataGenerator` adalah kelas di Keras yang digunakan untuk mempersiapkan dan mengaugmentasi gambar sebelum diberikan ke model.\n",
        "\n",
        "1. Parameter `rescale=1.0/255.0`:\n",
        "  \n",
        "  Mengubah skala nilai piksel gambar dari 0-255 menjadi 0-1 dengan membagi setiap nilai piksel dengan 255. Ini adalah langkah normalisasi yang umum dalam pengolahan citra.\n",
        "\n",
        "2. Parameter `validation_split=0.2`:\n",
        "\n",
        "  Menentukan bahwa 20% dari data akan digunakan untuk validasi, sedangkan 80% sisanya untuk pelatihan."
      ],
      "metadata": {
        "id": "a3jBcpXlYL71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset untuk pelatihan dan validasi\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'  # Menggunakan subset pelatihan\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'  # Menggunakan subset validasi\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DzGxe7lWyIl",
        "outputId": "662ab5bb-3357-4283-aff1-94e950f55611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 442 images belonging to 6 classes.\n",
            "Found 109 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi `flow_from_directory` adalah metode dari ImageDataGenerator yang digunakan untuk memuat data gambar dari direktori yang terstruktur dengan baik. Data harus disusun dalam subdirektori, di mana nama subdirektori tersebut akan menjadi label kelas.\n",
        "\n",
        "1. Parameter `dataset_dir`:\n",
        "    \n",
        "  Path ke direktori utama dataset yang diatur dalam folder terpisah berdasarkan label buah catur.\n",
        "\n",
        "2. Parameter `target_size=(150, 150)`:\n",
        "\n",
        "  Ukuran target untuk mengubah ukuran setiap gambar menjadi 150x150 piksel. Ini memastikan bahwa semua gambar memiliki ukuran yang konsisten saat dimasukkan ke dalam model.\n",
        "\n",
        "3. Parameter `batch_size=32`:\n",
        "\n",
        "  Jumlah gambar yang akan diproses sekaligus dalam satu batch. Batch size mempengaruhi penggunaan memori dan performa pelatihan.\n",
        "\n",
        "4. Parameter `class_mode='categorical'`:\n",
        "\n",
        "  Menentukan bahwa label kelas akan diubah menjadi one-hot encoded vectors yang digunakan untuk klasifikasi multi-kelas.\n",
        "\n",
        "5. Parameter `subset='training'` dan `subset='validation'`:\n",
        "\n",
        "  Menentukan apakah subset data ini digunakan untuk pelatihan atau validasi berdasarkan validation_split yang telah diatur sebelumnya.\n",
        "\n",
        "Variable `train_generator`: Generator yang menghasilkan batch data pelatihan dari direktori. Menggunakan 80% dari dataset berdasarkan validation_split.\n",
        "\n",
        "Variable `validation_generator`: Generator yang menghasilkan batch data validasi dari direktori. Menggunakan 20% dari dataset berdasarkan validation_split."
      ],
      "metadata": {
        "id": "0pwIxDpFY16J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 3: Membangun Model TensorFlow\n",
        "Selanjutnya, kita akan membangun arsitektur model menggunakan Sequential API dari Keras."
      ],
      "metadata": {
        "id": "hu1NBoryW7FD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membangun model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(train_generator.num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "vRibTKqYW82z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model `Sequential` adalah jenis model dalam Keras yang memungkinkan Anda untuk membangun model lapis demi lapis secara berurutan.\n",
        "\n",
        "1. `Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))`:\n",
        "\n",
        "  *   `Conv2D`: Lapisan konvolusi yang menerapkan 32 filter (kernel) dengan ukuran 3x3 pada gambar input.\n",
        "  *   `activation='relu'`: Fungsi aktivasi ReLU (Rectified Linear Unit) yang digunakan untuk memperkenalkan non-linearitas ke dalam model.\n",
        "  *   `input_shape=(150, 150, 3)`: Bentuk input gambar, di mana 150x150 adalah dimensi gambar, dan 3 adalah jumlah saluran warna (RGB).\n",
        "\n",
        "2. `MaxPooling2D(2, 2)`:\n",
        "\n",
        "  *   `MaxPooling2D`: Lapisan pooling yang mengurangi dimensi gambar dengan mengambil nilai maksimum dalam setiap blok 2x2. Ini membantu mengurangi ukuran data dan mengurangi overfitting.\n",
        "\n",
        "3. `Conv2D(64, (3, 3), activation='relu')`:\n",
        "\n",
        "  *   Lapisan konvolusi kedua dengan 64 filter dan ukuran kernel 3x3.\n",
        "\n",
        "4. `MaxPooling2D(2, 2)`:\n",
        "\n",
        "  *   Lapisan pooling kedua dengan blok 2x2.\n",
        "\n",
        "5. `Conv2D(128, (3, 3), activation='relu')`:\n",
        "\n",
        "  *   Lapisan konvolusi ketiga dengan 128 filter dan ukuran kernel 3x3.\n",
        "\n",
        "6. `MaxPooling2D(2, 2)`:\n",
        "\n",
        "  *   Lapisan pooling ketiga dengan blok 2x2.\n",
        "\n",
        "7. `Flatten()`:\n",
        "\n",
        "  *   `Flatten`: Mengubah output dari lapisan sebelumnya menjadi vektor satu dimensi. Ini diperlukan sebelum memasukkannya ke dalam lapisan Dense.\n",
        "\n",
        "8. `Dense(512, activation='relu')`:\n",
        "\n",
        "  *   `Dense`: Lapisan fully connected dengan 512 neuron dan fungsi aktivasi ReLU. Lapisan ini berfungsi untuk menggabungkan fitur-fitur yang telah diekstraksi oleh lapisan konvolusi sebelumnya.\n",
        "  \n",
        "9. `Dropout(0.5)`:\n",
        "\n",
        "  *   `Dropout`: Lapisan dropout dengan probabilitas 50%. Dropout membantu mencegah overfitting dengan mengabaikan (meng-drop) neuron secara acak selama pelatihan.\n",
        "\n",
        "10. `Dense(train_generator.num_classes, activation='softmax')`:\n",
        "\n",
        "  *   Lapisan output dengan jumlah neuron sama dengan jumlah kelas dalam dataset (diambil dari `train_generator.num_classes`), menggunakan fungsi aktivasi softmax untuk menghasilkan probabilitas untuk setiap kelas.\n",
        "\n"
      ],
      "metadata": {
        "id": "pdouSoHGa8Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengompilasi model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "uWEnY_ChW_sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 4: Melatih Model\n",
        "Sekarang kita akan melatih model dengan data yang telah kita siapkan."
      ],
      "metadata": {
        "id": "jy9m9ZwFXNRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5TL0sHrXOej",
        "outputId": "9fc6711c-51eb-4d66-e31d-dc89973c2882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "13/13 [==============================] - 251s 19s/step - loss: 1.9702 - accuracy: 0.1488 - val_loss: 1.7740 - val_accuracy: 0.1875\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 36s 3s/step - loss: 1.7235 - accuracy: 0.2732 - val_loss: 1.7478 - val_accuracy: 0.2083\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 33s 2s/step - loss: 1.6165 - accuracy: 0.3585 - val_loss: 1.7818 - val_accuracy: 0.2604\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 34s 3s/step - loss: 1.4273 - accuracy: 0.4268 - val_loss: 1.7135 - val_accuracy: 0.3438\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 34s 3s/step - loss: 1.1836 - accuracy: 0.5585 - val_loss: 1.7543 - val_accuracy: 0.3750\n",
            "Epoch 6/20\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.9744 - accuracy: 0.6561 - val_loss: 1.9388 - val_accuracy: 0.3542\n",
            "Epoch 7/20\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.8869 - accuracy: 0.6902 - val_loss: 2.0334 - val_accuracy: 0.3333\n",
            "Epoch 8/20\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.7478 - accuracy: 0.7244 - val_loss: 2.1002 - val_accuracy: 0.3854\n",
            "Epoch 9/20\n",
            "13/13 [==============================] - 41s 3s/step - loss: 0.5185 - accuracy: 0.8049 - val_loss: 2.6484 - val_accuracy: 0.4375\n",
            "Epoch 10/20\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.5111 - accuracy: 0.8195 - val_loss: 2.8042 - val_accuracy: 0.3438\n",
            "Epoch 11/20\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.4214 - accuracy: 0.8317 - val_loss: 2.3384 - val_accuracy: 0.4583\n",
            "Epoch 12/20\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.4198 - accuracy: 0.8854 - val_loss: 2.7010 - val_accuracy: 0.3854\n",
            "Epoch 13/20\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.3296 - accuracy: 0.8780 - val_loss: 2.5431 - val_accuracy: 0.4062\n",
            "Epoch 14/20\n",
            "13/13 [==============================] - 33s 3s/step - loss: 0.2457 - accuracy: 0.9195 - val_loss: 3.2044 - val_accuracy: 0.4375\n",
            "Epoch 15/20\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.3306 - accuracy: 0.8951 - val_loss: 3.0094 - val_accuracy: 0.3854\n",
            "Epoch 16/20\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.2118 - accuracy: 0.9341 - val_loss: 3.1047 - val_accuracy: 0.4167\n",
            "Epoch 17/20\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.1391 - accuracy: 0.9610 - val_loss: 3.3180 - val_accuracy: 0.4167\n",
            "Epoch 18/20\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.1714 - accuracy: 0.9537 - val_loss: 3.8258 - val_accuracy: 0.4375\n",
            "Epoch 19/20\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.1180 - accuracy: 0.9732 - val_loss: 3.5270 - val_accuracy: 0.4375\n",
            "Epoch 20/20\n",
            "13/13 [==============================] - 34s 3s/step - loss: 0.1286 - accuracy: 0.9610 - val_loss: 3.7546 - val_accuracy: 0.4271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 5: Evaluasi dan Validasi\n",
        "Kita akan mengevaluasi performa model dan menyimpan model yang telah dilatih."
      ],
      "metadata": {
        "id": "I4hIidXTdHdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengevaluasi model\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation loss: {loss}')\n",
        "print(f'Validation accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz7yBjM6dIia",
        "outputId": "3ad951c2-668a-4ed5-c144-7b21a3e26fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 4s 642ms/step - loss: 3.8270 - accuracy: 0.4220\n",
            "Validation loss: 3.8269615173339844\n",
            "Validation accuracy: 0.4220183491706848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menyimpan model\n",
        "# model.save('/content/drive/My Drive/model_catur.h5')"
      ],
      "metadata": {
        "id": "kssh-unDdLz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kesimpulan\n",
        "\n",
        "Berdasarkan hasil evaluasi yang Anda sebutkan, yaitu `Validation loss: 3.8269615173339844` dan `Validation accuracy: 0.4220183491706848`, bisa dikatakan bahwa model tersebut masih kurang baik. Dengan akurasi validasi sekitar 42%, model tersebut masih memiliki ruang untuk perbaikan yang signifikan.\n",
        "\n",
        "Sedingga itu perlu adanya perbaikan model, baik berupa perbaikan pada augmentasi data atau perbaikan pada arsitektur."
      ],
      "metadata": {
        "id": "DvMwrmYsdoAs"
      }
    }
  ]
}